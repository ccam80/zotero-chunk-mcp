| MDP Field | Model Element | Structure | Description |
| --- | --- | --- | --- |
| MDP.F | Negative variational free energy of each policy over time. | Rows = policies. Columns = time points. | Negative variational free energy of each policy at each time point in the trial. For example, if there are 2 policies and 6 time points there will be a 2 × 6 matrix containing the negative variational free energy of each policy at each point in the trial. |
| MDP.G | Negative expected free energy of each policy over time. | Rows = policies. Columns = time points. | Negative expected free energy of each policy at each time point in the trial. For example, if there are 2 policies and 6 time points there will be a 2 × 6 matrix containing the negative expected free energy of each policy at each point in the trial. |
| MDP.H | Total negative variational free energy over time. | Columns = time points. | Total negative variational free energy averaged across states and policies at each time point. For example, if there are 8 time points there will be a 1 × 8 row vector containing the total negative free energy at each time point. |
| MDP.Fa MDP.Fd MDP.Fb ... | MDP.Fa is the negative free energy of parameter 'a' (if learning A matrix). There are also analogous fields if learning other matrices/vectors (e.g., MDP.Fd for learning the parameters of the D vector, etc.). | Columns = one per outcome modality or hidden state factor (i.e., depending on the specific parameters being learned). If the agent is learning parameters of a single vector (e.g., E), this will be a single column. | KL divergence between the parameters of the matrix/vector that is being learned at the beginning of each trial and at the end of each trial. Each column in the vector may represent an outcome modality (i.e., in the case of the A matrix), a hidden state factor (i.e., in the case of the B matrix and D vector), or any other vector (e.g., the E vector). |
| MDP.O | Outcome vectors. | Rows = outcome modalities. Columns = time points. | Vectors (one per cell) specifying the outcomes for each modality at each time point. Observed outcomes are encoded as 1s, with 0s otherwise. |
| MDP.P | Probability of emitting an action. | Rows = one per controllable state factor. Columns = actions. Third dimension = time point. | The probability of emitting each particular action, expressed as a softmax function of a vector containing the probability of each action summed over each policy. For example, assume that there are two possible actions, with a posterior over policies of [.4 .4 .2], with policy 1 and 2 leading to action 1, and policy 3 leading to action 2. The probability of action 1 and 2 is therefore [.8 .2]. This vector is then passed through another softmax function controlled by the inverse temperature parameter α, which by default is extremely large (α = 512). Actions are then sampled |
