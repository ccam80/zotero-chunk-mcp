Model update
component
Update equation
Explanation
Model-specific description for
explore–exploit task (described
in detail Section 3)
Expected free energy of
each allowable policy
Gπ = DKL [q(o|π) ∥p (o|C)]
+ Eq(s|π) [H [p(o|s)]]
Gπ =
∑
τ
(
Asπ,τ · (
ln Asπ,τ −ln Cτ
)
−diag (
AT ln A)
· sπ,τ
)
The first equation reproduces the ‘risk + ambiguity’
expression for the expected free energy of each policy
(Gπ ) that is explained in the main text. The second
equation shows this same expression in terms of the
elements in the POMDP model used in this tutorial (i.e.,
in matrix notation).
Expected free energy evaluates the value of each policy
based on their expected ability to: (1) generate the
most desired outcomes, and (2) minimize uncertainty
about hidden states. Achieving the most desired
outcomes corresponds to minimizing the KL divergence
between preferred observations, p (o|C) = Cτ , and the
observations expected under each policy,
q (o|π) = Asπ,τ = oπ,t. Minimizing uncertainty
corresponds to minimizing the expected entropy of the
likelihood (Eq(s|π) [H [p(o|s)]] = −diag (
AT ln A)
· sπ,τ ).
Note that the diag() function simply takes the diagonal
elements of a matrix and places them in a row vector.
This is simply a convenient method for extracting and
operating on the correct matrix entries to calculate the
entropy, H [p (o|s)] = −∑
p(o|s) ln p (o|s), of the
distributions encoded within each column in A. For
simple numerical examples of calculating the risk and
ambiguity terms, see discussion of ‘outcome prediction
errors’ in Section 2.4.
The ‘risk’ term –
DKL [q(o|π) ∥p (o|C)] =
Asπ,τ · (
ln Asπ,τ −ln Cτ
)
–
drives the agent to select the
slot machine expected to be
most likely to pay out. If the
value of winning money in Cτ
is high enough (i.e., if p (o|C) is
sufficiently precise), this will
deter the agent from choosing
to ask for the hint.
The ‘ambiguity’ term –
Eq(s|π) [H [p(o|s)]] =
−diag (
AT ln A)
· sπ,τ – drives
the agent to minimize
uncertainty by asking for the
hint.
Marginal free energy of
each allowable policy
Fπ = Eq(s|π) [ln q (s|π)
−1
2
Eq(sτ−1|π)[ln p (sτ |sτ−1, π)]
−1
2
Eq(sτ+1|π)[ln p (sτ |sτ+1, π)]
−ln p (oτ |sτ )]
Fπ =
∑
τ
sπ,τ · (
ln sπ,τ
−1
2
(
ln(Bπ,τ−1sπ,τ−1)
+ ln(B†
π,τ sπ,τ+1))
−ln AToτ
)
The first equation shows the marginal (as opposed to
variational) free energy, which is now used in the most
recent implementations of active inference. The second
equation shows this same expression in terms of the
elements in the POMDP model used in this tutorial (i.e.,
in matrix notation). Marginal free energy has a sightly
different form than the expressions for VFE that are also
shown in the text (and which have been used in many
previous papers in the active inference literature). This
updated form improves on certain limitations of the
message passing algorithms derived from minimization
of VFE (see Section 2.3; also see (Parr, Markovic, Kiebel,
& Friston, 2019).
Marginal free energy evaluates the evidence that
inferred states provide for each policy (based on new
observations at each time point). See the first two rows
in this table on updating beliefs about states for an
l
i
f h
h
i
h
i
l
This would encode the amount
of surprise (given a choice of
policy) when observing a hint
or a win/loss after selecting a
specific slot machine.
