| Model update component | Update equation | Explanation | Model-specific description for explore–exploit task (described in detail Section 3) |
|---|---|---|---|
| Expected free energy of each allowable policy | G_π = D_{KL} [q(o\|π) \|\| p(o\|C)] + E_{q(s\|π)} [H [p(o\|s)]] G_π = ∑_τ (A s_{π,τ} · (ln A s_{π,τ} − ln C_τ) − diag(A^T ln A) · s_{π,τ}) | The first equation reproduces the 'risk + ambiguity' expression for the expected free energy of each policy (G_π) that is explained in the main text. The second equation shows this same expression in terms of the elements in the POMDP model used in this tutorial (i.e., in matrix notation). Expected free energy evaluates the value of each policy based on their expected ability to: (1) generate the most desired outcomes, and (2) minimize uncertainty about hidden states. Achieving the most desired outcomes corresponds to minimizing the KL divergence between preferred observations, p(o\|C) = C_τ, and the observations expected under each policy, q(o\|π) = A s_{π,τ} = o_{π,t}. Minimizing uncertainty corresponds to minimizing the expected entropy of the likelihood (E_{q(s\|π)} [H [p(o\|s)]] = − diag(A^T ln A) · s_{π,τ}). Note that the diag() function simply takes the diagonal elements of a matrix and places them in a row vector. This is simply a convenient method for extracting and operating on the correct matrix entries to calculate the entropy, H [p(o\|s)] = − ∑ p(o\|s) ln p(o\|s), of the distributions encoded within each column in A. For simple numerical examples of calculating the risk and ambiguity terms, see discussion of 'outcome prediction errors' in Section 2.4. | The 'risk' term – D_{KL} [q(o\|π) \|\| p(o\|C)] = A s_{π,τ} · (ln A s_{π,τ} − ln C_τ) – drives the agent to select the slot machine expected to be most likely to pay out. If the value of winning money in C_τ is high enough (i.e., if p(o\|C) is sufficiently precise), this will deter the agent from choosing to ask for the hint. The 'ambiguity' term – E_{q(s\|π)} [H [p(o\|s)]] = − diag(A^T ln A) · s_{π,τ} – drives the agent to minimize uncertainty by asking for the hint. |
| Marginal free energy of each allowable policy | F_π = E_{q(s\|π)} [ln q(s\|π) − 1/2 E_{q(s_{τ-1}\|π)}[ln p(s_τ\|s_{τ-1}, π)] − 1/2 E_{q(s_{τ+1}\|π)}[ln p(s_τ\|s_{τ+1}, π)] − ln p(o_τ\|s_τ)] F_π = ∑_τ s_{π,τ} · (ln s_{π,τ} − 1/2 (ln(B_{π,τ-1} s_{π,τ-1}) + ln(B^†_{π,τ} s_{π,τ+1})) − ln A^T o_τ) | The first equation shows the marginal (as opposed to variational) free energy, which is now used in the most recent implementations of active inference. The second equation shows this same expression in terms of the elements in the POMDP model used in this tutorial (i.e., in matrix notation). Marginal free energy has a slightly different form than the expressions for VFE that are also shown in the text (and which have been used in many previous papers in the active inference literature). This updated form improves on certain limitations of the message passing algorithms derived from minimization of VFE (see Section 2.3; also see (Parr, Markovic, Kiebel, & Friston, 2019). Marginal free energy evaluates the evidence that inferred states provide for each policy (based on new observations at each time point). See the first two rows in this table on updating beliefs about states for an | This would encode the amount of surprise (given a choice of policy) when observing a hint or a win/loss after selecting a specific slot machine. |
