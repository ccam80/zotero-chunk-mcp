Model variable*
General definition
Model specification for explore–exploit task (described in
detail Section 3)
D vector: p(s1)
A vector encoding beliefs about (a probability distribution
over) initial hidden states. When there is more than one
hidden state factor, there is one D vector per state factor.
The agent begins in an initial state of maximal uncertainty
about the context state (prior to learning), but complete
certainty that it will start in the ‘start’ choice state.
E vector: p(π)
A distribution encoding beliefs about what policies will be
chosen a priori (a prior probability distribution over policies,
implemented as a vector assigning one value to each policy),
based on the number of times different actions have been
chosen in the past.
The agent has no initial habits to choose one slot machine or
another (prior to learning).
*While, for consistency, we have used the standard notation found in the active inference literature, it is important to note that it does not always clearly distinguish
between distributions and the possible values taken by random variables under those distributions. For example, π refers to the distribution over policies, but when
used as a subscript it indexes each individual policy (e.g., Bπ,τ indicates a distinct matrix for each different policy). This same convention holds for s and o.
