| Model update component | Update equation | Explanation | Model-specific description for explore–exploit task (described in detail Section 3) |
| --- | --- | --- | --- |
| Expected free energy precision | p(γ) = Γ(1, β) E[γ] = γ = 1/β Iterated to convergence: π_0 ← σ(ln **E** − γ**G**) π ← σ(ln **E** − **F** − γ**G**) G_error ← (π − π_0) · (−**G**) β_update ← β − β_0 + G_error β ← β − β_update/ψ γ ← 1/β | The β term, and its prior value β_0, is a hyperparameter on the expected free energy precision term (γ). Specifically, β is the 'rate' parameter of a gamma distribution (Γ) with a 'shape' parameter value of 1. The expected value of this distribution, E[γ] = γ, is equal to the reciprocal of β. Note that we use the non-italicized γ to refer to the random variable and use the italicized γ to refer to the scalar value of that variable. This scalar is what is subsequently updated based on the equations shown here. The γ term controls the precision of G, based on the agent's confidence in its estimates of expected free energy. This confidence changes when new observations are consistent or inconsistent with G. More specifically, γ modulates the influence of G on policy selection based upon a G prediction error (G_error). This is calculated based on the difference between the initial distribution over policies (π_0) and the posterior distribution after making a new observation (π). The difference between these terms reflects the extent to which new observations (scored by **F**) make policies more or less likely. If the vector encoding the posterior over policies increases in magnitude in comparison to the prior, and still points in the same direction, the difference vector between the posterior and the prior will point in the same direction as the −**G** vector (i.e., less than a 90° angle apart; see Fig. 9). If so, the value of γ will increase, thereby increasing the impact of G on policy selection. In contrast, if the difference vector between the posterior and the prior does not point in the same direction as the −**G** vector (i.e., greater than a 90° angle apart), γ will decrease and thereby reduce the impact of G on policy selection (i.e., as the agent's confidence in its estimates of expected free energy has decreased). Note that the β_update term mediating these updates technically corresponds to the gradient of free energy with respect to γ (∇_γ F). The subsequent update in the value of γ is such that G contributes to the posterior over policies in an optimal manner. β and G_error are often discussed in relation to dopamine in the active inference literature. Note that β_0 is the initial prior (which is not updated), | A higher value for β would reduce an agent's confidence in the best policy based on the values in **G**. This might lead the agent to select a slot machine more randomly or based to a greater extent on its past choices (i.e., if it has a precise prior over policies in the vector **E**). |
