| Model update component | Update equation | Explanation | Model-specific description for explore–exploit task (described in detail Section 3) |
|---|---|---|---|
| Updating beliefs about initial states expected under each allowable policy. | ε_{π,τ=1} ← 1/2 (ln D + ln(B^†_{π,τ} s_{π,τ+1})) + ln A^T o_τ − ln s_{π,τ=1} s_{π,τ=1} = σ( 1/2 (ln D + ln(B^†_{π,τ} s_{π,τ+1})) + ln A^T o_τ ) | First equation: The variable ε_{π,τ=1} is the state prediction error with respect to the first time point in a trial. Minimizing this error corresponds to minimizing VFE (via gradient descent) and is used to update posterior beliefs over states. The term (ln D + ln(B^†_{π,τ} s_{π,τ+1})) corresponds to prior beliefs in Bayesian inference, based on beliefs about the probability of initial states, D, and the probability of transitions to future states under a policy, ln(B^†_{π,τ} s_{π,τ+1}). The term A^T o_τ corresponds to the likelihood term in Bayesian inference, evaluating how consistent observed outcomes are with each possible state. The term ln s_{π,τ=1} corresponds to posterior beliefs over states (for the first time point in a trial) at the current update iteration. Second Equation: We move to the solution for the posterior s_{π,τ=1} by setting ε_{π,τ=1} = 0, solving for ln s_{π,τ=1}, and then taking the softmax (normalized exponential) function (denoted σ) to ensure that the posterior over states is a proper probability distribution with non-negative values that sums to 1. This equation is described in more detail in the main text. A numerical example of the softmax function is also shown in Appendix A. | Updating beliefs about: 1. Whether the left vs. right slot machine is more likely to pay out on a given trial. 2. The initial choice state (here, always the 'start' state). |
| Updating beliefs about all states after the first time point in a trial that are expected under each allowable policy. | ε_{π,τ>1} ← 1/2 (ln (B_{π,τ-1} s_{π,τ-1}) + ln (B^†_{π,τ} s_{π,τ+1})) + ln A^T o_τ − ln s_{π,τ>1} s_{π,τ>1} = σ( 1/2 (ln(B_{π,τ-1} s_{π,τ-1}) + ln(B^†_{π,τ} s_{π,τ+1})) + ln A^T o_τ ) | First equation: The variable ε_{π,τ>1} is the state prediction error with respect to all time points in a trial after the first time point. Minimizing this error corresponds to minimizing VFE (via gradient descent) and is used to update posterior beliefs over states. The term (ln(B_{π,τ-1} s_{π,τ-1}) + ln(B^†_{π,τ} s_{π,τ+1})) corresponds to prior beliefs in Bayesian inference, based on beliefs about the probability of transitions from past states, ln(B_{π,τ-1} s_{π,τ-1}), and the probability of transitions to future states, ln(B^†_{π,τ} s_{π,τ+1}), under a policy. The term ln A^T o_τ corresponds to the likelihood term in Bayesian inference, evaluating how consistent observed outcomes are with each possible state. Second Equation: As in the previous row, we move to the solution for the posterior, s_{π,τ>1}, by setting ε_{π,τ>1} = 0, solving for ln s_{π,τ>1}, and then taking the softmax function (σ). This equation is described in more detail in the main text. | Updating beliefs about: 1. Whether the left vs. right slot machine is more likely to pay out on a given trial. 2. Beliefs about choice states after the initial time point (here, this depends on the choice to take the hint or to select one of the slot machines). |
