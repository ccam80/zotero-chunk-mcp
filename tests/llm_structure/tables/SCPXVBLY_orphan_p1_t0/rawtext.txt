MRC Cognition and Brain Sciences Unit, University of Cambridge, Cambridge, UK
a r t i c l e
i n f o
Article history:
Received 1 March 2021
Received in revised form 11 September 2021
Accepted 15 November 2021
Available online 4 February 2022
Keywords:
Active inference
Computational neuroscience
Bayesian inference
Learning
Decision-making
Machine learning
a b s t r a c t
The active inference framework, and in particular its recent formulation as a partially observable
Markov decision process (POMDP), has gained increasing popularity in recent years as a useful
approach for modeling neurocognitive processes. This framework is highly general and flexible in
its ability to be customized to model any cognitive process, as well as simulate predicted neuronal
responses based on its accompanying neural process theory. It also affords both simulation experiments
for proof of principle and behavioral modeling for empirical studies. However, there are limited
resources that explain how to build and run these models in practice, which limits their widespread
use. Most introductions assume a technical background in programming, mathematics, and machine
learning. In this paper we offer a step-by-step tutorial on how to build POMDPs, run simulations using
standard MATLAB routines, and fit these models to empirical data. We assume a minimal background
in programming and mathematics, thoroughly explain all equations, and provide exemplar scripts that
can be customized for both theoretical and empirical studies. Our goal is to provide the reader with the
requisite background knowledge and practical tools to apply active inference to their own research.
We also provide optional technical sections and multiple appendices, which offer the interested reader
additional technical details. This tutorial should provide the reader with all the tools necessary to use
these models and to follow emerging advances in active inference research.
© 2021 The Author(s). Published by Elsevier Inc. This is an open access article under the CC BY-NC-ND
license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
Introduction
Active
inference, and in particular its recent application to
partially observable Markov decision processes (POMDPs; de-
fined below), offers a unified mathematical framework for mod-
eling perception, learning, and decision making (Da Costa, Parr
et al., 2020; Friston, Parr, & de Vries, 2017c; Friston, Rosch, Parr,
Price, & Bowman, 2018; Parr & Friston, 2018b). This framework
treats each of these psychological processes, and their interac-
tions, as interdependent forms of inference. Namely, decision-
making agents are assumed to infer the probability of different
external states and events in the environment – including their
own actions – by combining prior beliefs with sensory input.
Unlike ‘passive’, perceptual inference processes (e.g., inferring
impinging on the retina), the inferences underlying decision-
making are ‘active’, in the sense that the agent infers the ac-
tions most likely to generate preferred sensory input (e.g., in-
ferring that eating some food will reduce a feeling of hunger).
Agents also infer the actions most likely to reduce uncertainty
and facilitate learning (e.g., inferring that opening the fridge
will reveal available food options). This leads decision-making
to favor actions that optimize a trade-off between maximiz-
ing reward and information gain. The resulting patterns of per-
ception and behavior predicted by active inference match well
with those observed empirically (e.g., see Smith et al., 2021d,
2021c, 2020b; Smith, Kuplicki, Teed, Upshaw, & Khalsa, 2020c;
Smith et al., 2021e, 2020e). The neural process theory associated
with active inference has also successfully reproduced empiri-
cally observed neural responses in multiple research paradigms
