{
  "table_id": "SCPXVBLY_table_1",
  "paper": "active-inference-tutorial",
  "item_key": "SCPXVBLY",
  "page_num": 15,
  "table_index": 1,
  "caption": "Table 1 Model variables.",
  "headers": [
    "Model variable*",
    "General definition",
    "Model specification for explore–exploit task (described in detail Section 3)"
  ],
  "rows": [
    [
      "oτ",
      "Observable outcomes at time τ.",
      "Outcome modalities:\n1. Hints (no hint, hint-left, hint-right)\n2. Reward (start, lose, win)\n3. Observed behavior (start, take hint, choose left, choose right)"
    ],
    [
      "sτ",
      "Hidden states at time τ. One vector of possible state values for each state factor (i.e., each independent set of states; e.g., visual vs. auditory states).",
      "Hidden state factors:\n1. Context (left machine is better vs. right machine is better)\n2. Choices (start, take hint, choose left, choose right)"
    ],
    [
      "π",
      "A vector encoding the distribution over policies reflecting the predicted value of each policy. Each policy is a series of allowable actions in a vector U, where actions correspond to different state transitions (i.e., different Bπ,τ matrices) that can be chosen by the agent for each state factor. Policies are chosen by sampling from this distribution.",
      "Allowable policies include the decision to:\n1. Stay in the start state\n2. Get the hint and then choose the left machine\n3. Get the hint and then choose the right machine\n4. Immediately choose the left machine (and then return to the start state)\n5. Immediately choose the right machine (and then return to the start state)"
    ],
    [
      "A matrix: p(oτ|sτ)",
      "A matrix encoding beliefs about the relationship between hidden states and observable outcomes at each time point τ (i.e., the probability that specific outcomes will be observed given specific hidden states at specific times). Note that in the POMDP structure typically used in the active inference literature (and which we describe in this tutorial), the likelihood is assumed to remain constant across time points in a trial, and hence will not differ at different values for τ (although one could adjust this if so desired). The likelihood is also assumed to be identical across policies, and so there is no indexing with respect to π.\nWhen there is more than one outcome modality, there is one A matrix per outcome modality. When there is more than one state factor, these matrices become high-dimensional and are technically referred to as tensors. For example, a second state factor corresponds to a third matrix dimension, a third state factor corresponds to a fourth matrix dimension, and so forth.",
      "Encodes beliefs about the relationship between:\n1. Probability that the hint is accurate in each context\n2. Probability of reward in each context\n3. Identity mapping between choice states and observed behavior"
    ],
    [
      "Bπ,τ matrix: p(sτ+1|sτ, π)",
      "A matrix encoding beliefs about how hidden states will evolve over time (transition probabilities). For states that are under the control of the agent, there are multiple Bπ,τ matrices, where each matrix corresponds to one action (state transition) that the agent may choose at a given time point (if consistent with an allowable policy). When there is more than one hidden state factor, there is one or more Bπ,τ matrices per state factor (depending on policies).",
      "Encodes beliefs that:\n1. Context does not change within a trial\n2. Transitions from any choice state to any other are possible, depending on the policy."
    ],
    [
      "C matrix: p(oτ|C)",
      "A matrix encoding the degree to which some observed outcomes are preferred over others (technically modeled as prior expectations over outcomes). When there is more than one outcome modality, there is one C matrix per outcome modality. Rows indicate possible observations; columns indicate time points. Note that each column of values in C is passed through a softmax function (transforming it into a proper probability distribution) and then log-transformed (using the natural log). Thus, preferences become log-probabilities over outcomes.",
      "Encodes the stronger preference for wins than losses. Wins are also more preferred at the second time point than the third time point."
    ]
  ],
  "notes": "Table continues on next page. The header 'Model specification for explore-exploit task (described in detail Section 3)' spans column 3. Row content with numbered lists represents multi-line cell content joined with spaces; newlines used here for readability only.",
  "verified": true,
  "footnotes": ""
}
