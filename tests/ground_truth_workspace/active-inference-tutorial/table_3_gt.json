{
  "table_id": "SCPXVBLY_table_2",
  "paper": "active-inference-tutorial",
  "item_key": "SCPXVBLY",
  "page_num": 18,
  "table_index": 3,
  "caption": "Table 2 Matrix formulation of equations used for inference.",
  "headers": [
    "Model update component",
    "Update equation",
    "Explanation",
    "Model-specific description for explore–exploit task (described in detail Section 3)"
  ],
  "rows": [
    [
      "Updating beliefs about initial states expected under each allowable policy.",
      "επ,τ=1 ← ½(ln D + ln(B†π,τ sπ,τ+1)) + ln Aᵀoτ − ln sπ,τ=1 sπ,τ=1 = σ(½(ln D + ln(B†π,τ sπ,τ+1)) + ln Aᵀoτ)",
      "First equation: The variable επ,τ=1 is the state prediction error with respect to the first time point in a trial. Minimizing this error corresponds to minimizing VFE (via gradient descent) and is used to update posterior beliefs over states. The term (ln D + ln(B†π,τ sπ,τ+1)) corresponds to prior beliefs in Bayesian inference, based on beliefs about the probability of initial states, D, and the probability of transitions to future states under a policy, ln(B†π,τ sπ,τ+1). The term Aᵀoτ corresponds to the likelihood term in Bayesian inference, evaluating how consistent observed outcomes are with each possible state. The term ln sπ,τ=1 corresponds to posterior beliefs over states (for the first time point in a trial) at the current update iteration. Second Equation: We move to the solution for the posterior sπ,τ=1 by setting επ,τ=1 = 0, solving for ln sπ,τ=1, and then taking the softmax (normalized exponential) function (denoted σ) to ensure that the posterior over states is a proper probability distribution with non-negative values that sums to 1. This equation is described in more detail in the main text. A numerical example of the softmax function is also shown in Appendix A.",
      "Updating beliefs about: 1. Whether the left vs. right slot machine is more likely to pay out on a given trial. 2. The initial choice state (here, always the ‘start’ state)."
    ],
    [
      "Updating beliefs about all states after the first time point in a trial that are expected under each allowable policy.",
      "επ,τ>1 ← ½(ln(Bπ,τ−1 sπ,τ−1) + ln(B†π,τ sπ,τ+1)) + ln Aᵀoτ − ln sπ,τ>1 sπ,τ>1 = σ(½(ln(Bπ,τ−1 sπ,τ−1) + ln(B†π,τ sπ,τ+1)) + ln Aᵀoτ)",
      "First equation: The variable επ,τ>1 is the state prediction error with respect to all time points in a trial after the first time point. Minimizing this error corresponds to minimizing VFE (via gradient descent) and is used to update posterior beliefs over states. The term (ln(Bπ,τ−1 sπ,τ−1) + ln(B†π,τ sπ,τ+1)) corresponds to prior beliefs in Bayesian inference, based on beliefs about the probability of transitions from past states, ln(Bπ,τ−1 sπ,τ−1), and the probability of transitions to future states, ln(B†π,τ sπ,τ+1), under a policy. The term ln Aᵀoτ corresponds to the likelihood term in Bayesian inference, evaluating how consistent observed outcomes are with each possible state. Second Equation: As in the previous row, we move to the solution for the posterior, sπ,τ>1, by setting επ,τ>1 = 0, solving for ln sπ,τ>1, and then taking the softmax function (σ). This equation is described in more detail in the main text.",
      "Updating beliefs about: 1. Whether the left vs. right slot machine is more likely to pay out on a given trial. 2. Beliefs about choice states after the initial time point (here, this depends on the choice to take the hint or to select one of the slot machines)."
    ],
    [
      "Probability of selecting each allowable policy",
      "π0 = σ(ln E − γG) π = σ(ln E − F − γG)",
      "The initial distribution over policies before making any observations (π0), and the posterior distribution over policies after an observation (π). The initial distribution is made up of the learned prior over policies encoded in the E vector (reflecting the number of times a policy has previously been chosen) and the expected free energy of each allowable policy (G). The posterior distribution is determined by E, G, and the variational free energy (F) under each policy after making a new observation. The influence of G is also modulated by an expected precision term (γ), which encodes prior confidence in beliefs about G (described further in the main text; also see Fig. 9). See row 1 for an explanation of the function of the σ symbol. We note, however, that incorporation of E, F, and/or γ when computing π is a modeling choice. These need not be included in all cases (e.g., see top-left portion of Fig. 5; also see Da Costa, Parr et al., 2020). In some contexts, one might choose to include some of these terms but not others, or to only include G. This depends on the research question. (e.g., E will be useful if task behavior is influenced by habits, while F/γ can be useful when there are many possible deep policies to choose from). See the row in this table on ‘Expected free energy precision’ for more details about inference over policies when F/γ are included. This is also discussed further in the main text.",
      "Updating overall beliefs about whether the best course of action is to take the hint and/or to choose the left vs. right slot machine."
    ]
  ],
  "notes": "Table continues on next page. Equations contain mathematical notation. B† denotes the transpose of B (dagger notation). Aᵀ denotes A-transpose. ½ denotes one-half.",
  "verified": true,
  "footnotes": ""
}
