{
  "table_id": "SCPXVBLY_table_2",
  "paper": "active-inference-tutorial",
  "item_key": "SCPXVBLY",
  "page_num": 18,
  "table_index": 3,
  "caption": "Table 2 Matrix formulation of equations used for inference.",
  "headers": ["Model update component", "Update equation", "Explanation", "Model-specific description for explore-exploit task (described in detail Section 3)"],
  "rows": [
    ["Updating beliefs about initial states expected under each allowable policy", "s\u03c4=1,\u03c0 = \u03c3(ln D + ln\u1d40 A\u1d52 o\u03c4=1)\ns\u03c4=1,\u03c0 \u2190 \u03c3( (ln D + \u1d40A\u1d52 ln s\u03c4=2,\u03c0) )\n\u22ee\n\u03c4=1 \u2190 \u03c3( \u00bc (ln D + \u1d40A\u1d52 ln s\u03c4+1,\u03c0 + lnA\u1d52 o\u03c4=1) )", "First equation: The variable s\u03c4=1,\u03c0 is the state prediction error with respect to the first time point in a trial. Minimizing this error corresponds to minimizing VFE (via gradient descent) and is used to update posterior beliefs over states. The term (ln D + ln\u1d40 A\u03c4=1) corresponds to prior beliefs in Bayesian inference, based on beliefs about the probability of initial hidden states, D, and the probability of transitions to future states under a policy, ln\u1d40 B\u03c4,\u03c0 s\u03c4+1. The term A\u1d52 o\u03c4 corresponds to the likelihood term in Bayesian evaluation of the most consistent observed outcomes with each possible state. The term ln s\u03c4+1,\u03c0 corresponds to posterior beliefs over states (for the first time point in a trial) at the current update iteration.\nSecond Equation: We move to the solution for the posterior, s\u03c4=1,\u03c0, by setting \u03b5\u03c4=1 = 0, solving for ln s\u03c4=1,\u03c0, and then taking the softmax (normalized exponential) function (denoted \u03c3) to ensure that the posterior over states is a proper probability distribution with non-negative values that sums to 1. This equation is described in more detail in the main text. A numerical example of the softmax function is also shown in Appendix A.", "Updating beliefs about:\n1. Whether the left vs. right slot machine is more likely to pay out on a given trial.\n2. The initial choice state (here, always the \u2018start\u2019 state)."],
    ["Updating beliefs about all states after the first time point in a trial that are expected under each allowable policy", "s\u03c4,\u03c0 = \u03c3(ln\u1d40 B\u03c4\u22121,\u03c0 s\u03c4\u22121,\u03c0 + lnA\u1d52 o\u03c4 + ln B\u03c4,\u03c0 s\u03c4+1,\u03c0)\n\u03b5\u03c4,\u03c0 \u2190 ln s\u03c4,\u03c0 \u2212 (ln\u1d40 B\u03c4\u22121,\u03c0 s\u03c4\u22121,\u03c0 + ln\u1d40 B\u03c4,\u03c0 s\u03c4+1,\u03c0 + lnA\u1d52 o\u03c4)\ns\u03c4,\u03c0 \u2190 \u03c3( (ln\u1d40 B\u03c4\u22121,\u03c0 s\u03c4\u22121,\u03c0 + lnA\u1d52 o\u03c4 + ln B\u03c4,\u03c0 s\u03c4+1,\u03c0) )", "First equation: The variable \u03b5\u03c4,\u03c0 is the state prediction error with respect to all states in a trial after the first time point. Minimizing this error corresponds to minimizing VFE (via gradient descent) and is used to update posterior beliefs over states. The term (ln\u1d40 B\u03c4\u22121,\u03c0 s\u03c4\u22121,\u03c0 + ln B\u03c4,\u03c0 s\u03c4+1,\u03c0) corresponds to prior beliefs in Bayesian inference, based on beliefs about the probability of transitions from past states, ln\u1d40 B\u03c4\u22121,\u03c0 s\u03c4\u22121,\u03c0, and the probability of transitions to future states under a policy, ln B\u03c4,\u03c0 s\u03c4+1,\u03c0. In A\u1d52 o\u03c4 corresponds to the likelihood term in Bayesian inference, evaluating how consistent the observations are with each possible state.\nSecond Equation: As in the previous row, we move to the solution for the posterior, s\u03c4,\u03c0, by setting \u03b5\u03c4,\u03c0 = 0, solving for ln s\u03c4,\u03c0, and then taking the softmax (\u03c3) function. This equation is described in more detail in the main text.", "Updating beliefs about:\n1. Whether the left vs. right slot machine is more likely to pay out on a given trial.\n2. Beliefs about choice states at each time point (here, this depends on the choice to take the hint or select one of the slot machines)."],
    ["Probability of selecting each allowable policy", "\u03c0\u03c0 \u221d \u03c3(ln E \u2212 \u03b3G)", "The initial distribution over policies before making any observations (\u03c0\u2080) and the posterior distribution over policies after an observation (\u03c0\u2081). The initial distribution is made up of the learned prior over policies encoded in the E vector (reflecting the number of times a policy has previously been chosen) and the expected free energy of each allowable policy (G). The posterior distribution is determined by F, G, and the variational free energy (F) under each policy after making a new observation. The influence of G is also modulated by the expected precision term (\u03b3) which encodes prior strength of belief in G. (described further in the main text; also see Fig. 9). See row 1 for an explanation of the function of the \u03c3 symbol.\nWe note, however, that incorporation of E, F, and/or \u03b3 when computing \u03c0 is a modeling choice. These need not be included in all cases (e.g., see top left portion of Fig. 9; also see Da Costa, Parr et al., 2020). In some contexts, one might choose to include some of these terms but not others, or to only include G. This depends on the research question (e.g., F will be useful if task behavior is influenced by habits, while F/\u03b3 can be useful when there are many possible deep policies to choose from). See the row in this table on \u2018Expected free energy precision\u2019 for more details about inference policies when F/\u03b3 are included. This is also discussed further in the main text.", "Updating overall beliefs about whether the best course of action is to take the hint and/or to choose the left vs. right slot machine."]
  ],
  "notes": "Table continues on next page. Equations contain mathematical notation including subscripts, superscripts, and Greek symbols. Multi-line cell content joined with spaces; newlines used here for readability.",
  "verified": false
}